{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Decision Trees and Random Forest\n",
    "\n",
    "### Name:\n",
    "### Course Level:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "* In this project, we explore the application of classification using: a) Decision Tree and b) Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "* The objective of this project is to develop software modules for classification of Titanic passengers via decision trees and random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first problem we aim to us Information Gain to build a decision tree for classification.  The tree will complete when one of two conditions are met:\n",
    "\n",
    "1. The maximum tree depth is reached.\n",
    "2. The minimum number of samples is reached in a given leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem A (70pts)**\n",
    "\n",
    "1 (5pts). The first thing you'll need to do is read the Titatinic dataset from D2L (details about the dataset can be found [Here](https://www.kaggle.com/c/titanic/data?select=test.csv)).  Note that you will need both the train.csv (to build your tree) and test.csv to evaluate the classificaiton accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Read in the data\n",
    "def load_data():\n",
    "    # Load training and test datasets\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    for i, df in enumerate([train_data, test_data]):\n",
    "        # Fill missing age values with the median age - fixed to avoid warnings\n",
    "        median_age = df['Age'].median()\n",
    "        df['Age'] = df['Age'].fillna(median_age)\n",
    "        \n",
    "        # Fill missing embarked values with the most common value - fixed\n",
    "        mode_embarked = df['Embarked'].mode()[0]\n",
    "        df['Embarked'] = df['Embarked'].fillna(mode_embarked)\n",
    "        \n",
    "        # Convert categorical features to numerical\n",
    "        df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "        \n",
    "        # Creating age groups to simplify the tree\n",
    "        df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 65, 100], \n",
    "                               labels=['Child', 'Teen', 'Adult', 'Elderly'])\n",
    "        \n",
    "        # Creating fare groups\n",
    "        df['FareGroup'] = pd.cut(df['Fare'], bins=[0, 7.91, 14.45, 31, 513], \n",
    "                                labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    # Select useful features\n",
    "    features = ['Pclass', 'Sex', 'AgeGroup', 'SibSp', 'Parch', 'FareGroup', 'Embarked']\n",
    "    \n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['Survived']\n",
    "    \n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['Survived']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (20pts). Next, we need a couple helper functions to compute the conditional entropy and information gain.\n",
    "\n",
    "**Recall: Entropy** \n",
    "$$\n",
    "    H(X) = -\\sum_{\\textbf{x} \\in X} p(\\textbf{x}) \\log_2( p(\\textbf{x}) )\n",
    "$$\n",
    "\n",
    "**Recall: Conditional Entropy** \n",
    "$$\n",
    "    H(Y|X=\\textbf{x}) = -\\sum_{\\textbf{y} \\in Y} p(\\textbf{y}|\\textbf{x}) \\log_2 ( p(\\textbf{y}|\\textbf{x}) )\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    p(\\textbf{y}|\\textbf{x}) = \\frac{p(\\textbf{y},\\textbf{x})}{p(\\textbf{x})}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    p(\\textbf{x}) = \\sum_{\\textbf{y} \\in Y} p(\\textbf{y},\\textbf{x})\n",
    "$$\n",
    "\n",
    "**Recall: Information Gain** \n",
    "$$\n",
    "    IG(Y|X) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions here (you may want to modify the input parameters depending on your implimentation #\n",
    "def Entropy(y):\n",
    "    counter = Counter(y)\n",
    "    total_samples = len(y)\n",
    "    entropy = 0\n",
    "    for count in counter.values():\n",
    "        probability = count / total_samples\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def C_Entropy(X, y, feature):\n",
    "    # Get all unique values of the feature\n",
    "    feature_values = set(X[feature])\n",
    "    total_samples = len(y)\n",
    "    conditional_entropy = 0\n",
    "    \n",
    "    for value in feature_values:\n",
    "        # Filter samples where feature = value\n",
    "        indices = X[feature] == value\n",
    "        subset_y = y[indices]\n",
    "        \n",
    "        weight = len(subset_y) / total_samples\n",
    "        \n",
    "        # Add weighted entropy of this subset\n",
    "        conditional_entropy += weight * Entropy(subset_y)\n",
    "    \n",
    "    return conditional_entropy\n",
    "\n",
    "def IG(X, y, feature):\n",
    "    original_entropy = Entropy(y)\n",
    "    cond_entropy = C_Entropy(X, y, feature)\n",
    "    information_gain = original_entropy - cond_entropy\n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (30pts). Define a function to build the tree, recall there should be one of two exit criterion:\n",
    "\n",
    "1. The maximum tree depth is reached\n",
    "2. The minimum number of samples is reached in a given leaf node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Tree (can be recursive) #\n",
    "def BuildTree(X, y, features, max_depth=3, min_samples=5, current_depth=0):\n",
    "    # Convert Series to array for easier handling\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    \n",
    "    # Check if stopping criteria are met\n",
    "    if current_depth >= max_depth or len(y) <= min_samples or len(set(y)) == 1:\n",
    "        # Return the most common class as leaf node\n",
    "        most_common_class = Counter(y).most_common(1)[0][0]\n",
    "        return {'type': 'leaf', 'class': most_common_class}\n",
    "    \n",
    "    # If no features left, return leaf node\n",
    "    if not features:\n",
    "        most_common_class = Counter(y).most_common(1)[0][0]\n",
    "        return {'type': 'leaf', 'class': most_common_class}\n",
    "    \n",
    "    # Find the best feature to split on\n",
    "    max_gain = -float('inf')\n",
    "    best_feature = None\n",
    "    \n",
    "    for feature in features:\n",
    "        gain = IG(X, y, feature)\n",
    "        if gain > max_gain:\n",
    "            max_gain = gain\n",
    "            best_feature = feature\n",
    "    \n",
    "    # If no good split found, create leaf node\n",
    "    if best_feature is None:\n",
    "        most_common_class = Counter(y).most_common(1)[0][0]\n",
    "        return {'type': 'leaf', 'class': most_common_class}\n",
    "    \n",
    "    # Create a decision node for the best feature\n",
    "    tree = {'type': 'node', 'feature': best_feature, 'children': {}}\n",
    "    \n",
    "    # Get unique values of the best feature\n",
    "    feature_values = set(X[best_feature])\n",
    "    \n",
    "    # Create subtrees for each value of the best feature\n",
    "    for value in feature_values:\n",
    "        # Filter samples where best_feature = value\n",
    "        indices = X[best_feature] == value\n",
    "        subset_X = X.loc[indices]\n",
    "        subset_y = y[indices] if isinstance(y, np.ndarray) else y.loc[indices]\n",
    "        \n",
    "        # Skip if no samples with this value\n",
    "        if len(subset_y) == 0:\n",
    "            tree['children'][value] = {'type': 'leaf', 'class': Counter(y).most_common(1)[0][0]}\n",
    "            continue\n",
    "        \n",
    "        # Recursively build subtree\n",
    "        remaining_features = [f for f in features if f != best_feature]\n",
    "        subtree = BuildTree(subset_X, subset_y, remaining_features, \n",
    "                          max_depth, min_samples, current_depth + 1)\n",
    "        \n",
    "        tree['children'][value] = subtree\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 (15pts). Next write a function to perform the prediciton (classificaiton) of a new training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict #\n",
    "def DT_predict(tree, sample):\n",
    "    # If leaf node, return the class\n",
    "    if tree['type'] == 'leaf':\n",
    "        return tree['class']\n",
    "    \n",
    "    # Get the feature value from the sample\n",
    "    feature = tree['feature']\n",
    "    value = sample[feature]\n",
    "    \n",
    "    # If this value wasn't seen during training, return the most common class\n",
    "    if value not in tree['children']:\n",
    "        # Find any child and get its class (assuming all leaves have same class)\n",
    "        for child in tree['children'].values():\n",
    "            if child['type'] == 'leaf':\n",
    "                return child['class']\n",
    "        # If no leaf found, just return 0 as default\n",
    "        return 0\n",
    "    \n",
    "    # Navigate to the next node based on the feature value\n",
    "    return DT_predict(tree['children'][value], sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 (5pts). Using the test set, evaluate the accuracy of your classification tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Accuracy #\n",
    "def accuracy(X_test, y_test, tree):\n",
    "    correct = 0\n",
    "    total = len(y_test)\n",
    "    \n",
    "    for i in range(total):\n",
    "        sample = X_test.iloc[i]\n",
    "        prediction = DT_predict(tree, sample)\n",
    "        if prediction == y_test.iloc[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem B (30pts)**\n",
    "* In Problem A, you designed a function to build a decision tree.  In this problem, you will use that function to build several different trees via bagging (bootstrapping and aggregation).  The result will be a random forest.  You should **at minimum** have a forest of at least 5 trees. \n",
    "\n",
    "* Once the forest is built, similar to A.5, design a function for comparing the accuracy of a standard tree, vs. random forest on the testing dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest #\n",
    "def bootstrap_sample(X, y):\n",
    "    n_samples = len(X)\n",
    "    indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
    "    return X.iloc[indices].reset_index(drop=True), y.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "def build_random_forest(X, y, features, n_trees=5, max_depth=3, min_samples=5):\n",
    "    forest = []\n",
    "    \n",
    "    for _ in range(n_trees):\n",
    "        # Create bootstrap sample\n",
    "        X_sample, y_sample = bootstrap_sample(X, y)\n",
    "        \n",
    "        # Select a random subset of features for this tree\n",
    "        n_features = max(1, int(len(features) * 0.7))  # Use 70% of features\n",
    "        sampled_features = random.sample(features, n_features)\n",
    "        \n",
    "        # Build a tree with this sample\n",
    "        tree = BuildTree(X_sample, y_sample, sampled_features, max_depth, min_samples)\n",
    "        \n",
    "        # Add tree to the forest\n",
    "        forest.append(tree)\n",
    "    \n",
    "    return forest\n",
    "\n",
    "def rf_predict(forest, sample):\n",
    "    predictions = [DT_predict(tree, sample) for tree in forest]\n",
    "    return Counter(predictions).most_common(1)[0][0]\n",
    "\n",
    "def rf_accuracy(X_test, y_test, forest):\n",
    "    correct = 0\n",
    "    total = len(y_test)\n",
    "    \n",
    "    for i in range(total):\n",
    "        sample = X_test.iloc[i]\n",
    "        prediction = rf_predict(forest, sample)\n",
    "        if prediction == y_test.iloc[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Visualization function for comparing DT and RF performance\n",
    "def plot_accuracy_comparison(dt_acc, rf_acc, n_trees):\n",
    "    models = ['Decision Tree', f'Random Forest ({n_trees} trees)']\n",
    "    accuracies = [dt_acc, rf_acc]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen'])\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, acc + 0.01, f'{acc:.4f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Titanic Classification: Decision Tree vs Random Forest')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('dt_vs_rf_accuracy.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7799\n",
      "Random Forest Accuracy (with 10 trees): 0.7632\n",
      "Improvement with Random Forest: -1.67%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load and preprocess data\n",
    "    X_train, y_train, X_test, y_test = load_data()\n",
    "    \n",
    "    # Get features\n",
    "    features = X_train.columns.tolist()\n",
    "    \n",
    "    # Build a single decision tree\n",
    "    decision_tree = BuildTree(X_train, y_train, features, max_depth=5, min_samples=10)\n",
    "    \n",
    "    # Calculate accuracy of the decision tree\n",
    "    dt_acc = accuracy(X_test, y_test, decision_tree)\n",
    "    print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
    "    \n",
    "    # Build a random forest\n",
    "    n_trees = 10  # Using 10 trees for better accuracy\n",
    "    random_forest = build_random_forest(X_train, y_train, features, n_trees=n_trees, \n",
    "                                      max_depth=5, min_samples=10)\n",
    "    \n",
    "    # Calculate accuracy of the random forest\n",
    "    rf_acc = rf_accuracy(X_test, y_test, random_forest)\n",
    "    print(f\"Random Forest Accuracy (with {n_trees} trees): {rf_acc:.4f}\")\n",
    "    \n",
    "    # Compare the results\n",
    "    print(f\"Improvement with Random Forest: {(rf_acc - dt_acc) * 100:.2f}%\")\n",
    "    \n",
    "    # Visualize the comparison\n",
    "    plot_accuracy_comparison(dt_acc, rf_acc, n_trees)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
